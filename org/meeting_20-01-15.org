* Meeting 15.01.2020
** Progress
*** Implemented uniform quantization for INQ
*** Implemented optimizer reset for STE
*** Fixed mistakes for quantization
- Main Problem: INQ schedule contained epoch 1: fraction 0, thus, calculated range at first epoch
*** Accuracy: 70.6%, compared with 71.0% without quantization
**** INQ: uniform, RPR, reset Opt, schedule:
| epoch | fraction |
|   500 |      0.5 |
|   510 |     0.75 |
|   520 |    0.875 |
|   530 |   0.9375 |
|   540 |  0.96875 |
|   550 |  0.98438 |
|   560 |  0.99219 |
|   570 |        1 |
**** STE: monitor epoch: 450, start epoch: 451, reset Opt,
**** 600 Epochs total
*** Comparison RPR to regular INQ: RPR is a bit better, about 0.16%
*** Note: still using AvgPool, MaxPool does perform worse (1% difference)
*** Ideas to improve (no time to implement / test)
**** Ignore outliers for determining quantization scale factor (for both INQ and STE)
**** optimize scale factor
** Questions:
*** How to do data loading? Interface with computer (UART?), or just define statically in a file (generated with python)
A: in source
*** Does example code exist?
A: Yes, ask Xiaying
*** PULP-DSP: How does ~PLP_MATH_LOOPUNROLL~ work? (Example: ~plp_mean_q32s_xpulpv2~):
#+BEGIN_SRC c
#if defined(PLP_MATH_LOOPUNROLL)

  tmpBS = (blockSize>>1);

  for (blkCnt=0; blkCnt<tmpBS; blkCnt++){
    sum1 += *pSrc;
    sum2 += *pSrc;
  }

  tmpBS = (blockSize%2U);

  for (blkCnt=0; blkCnt<tmpBS; blkCnt++){
    sum1 += *pSrc;
  }

#endif // PLP_MATH_LOOPUNROLL
#+END_SRC
A: It should have post increment. We do it twice because we have 2 cycles delay for load operations. this way, we have 100% utilization
*** Results should match exactly?
Answer: no, with epsilon of +- 1 quantization step
** Next Steps
*** Install environment (PULP-SDK)
*** Think about ordering the data
**** Might be different between different kernels, in order to make hardware loops more easy
**** Check load post increment Xpulp instructions 
[[https://www.pulp-platform.org/docs/ri5cy_user_manual.pdf][RI5CY documentation]], 14.1
*** Start Implementation
**** First Single Core, then paralellize
**** Do correct tooling, having one script which trains the network, exports the weights, prepares the wolf source code, compiles the source code, and load them onto the device / simulates it
** Notes
*** Mr Wolf & Pulp
**** FC and Cluster run entirely different programs, compiled differently
**** Try to do DMA's while we compute the data. Ideally, we never have to wait for the data
**** Write customized functions to optimize efficiency
**** Write static data to L2 with ~rt_l2_data~
*** Quantization
**** Do a sweep, checking number of bits for representation, what the performance is. Maybe increase time to train and retrain for lower accuracy
*** Other Resources for comparison
**** [[https://github.com/pulp-platform/pulp-nn][PULP-NN]] (not yet finished)
**** Gap-NN (Proprietary)
**** AutoTiler (Proprietary)
**** [[https://github.com/pulp-platform/fann-on-mcu][FANN-ON-MCU]]

