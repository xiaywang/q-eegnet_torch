* Meeting 20.01.2020
** Progress
*** Quantization Sweep
**** 4Bit quantization (8Bit Input & output): 70.2% accuracy (0.8% less than the full precision model)
*** Modifications to the Quantlab model (after thinking about the implementation on Mr Wolf)
**** Only quantize after conv + bn + relu + pool (because we can work on 32bit there)
**** Added output quantization layer
*** Derived transformation functions between the layers.
Idea: do relu, then pooling (by summation), then batchnorm and rescaling
*** Mr Wolf Project Start
**** Golden Model in Python with Numpy (with rounding, every output bit is exactly the same as the quantlab output)
**** Unit Test Framework
**** Copied Convolution and modified to only work on valid part (tested)
**** Implemented transform function to transform from layer to layer, from 32bit to 8bit (tested)
**** Implemented naive approach for first layer (tested, partially working)
Single core implementation, wait for every DMA copy to finish before calculating
** Planned next steps
*** implement naive approach for the entire network --> measure DMA wait time
*** improve by simultaneously copying and computing
*** Implement Parallel Computing
*** Implement different optimizations
** Ideas for Optimization
*** Combine layer 1 and 2
- Layer 1 does convolution only in time, layer 2 only in space (channels)
- Convolution for Layer 2 only needs (from the first convolution) the 22 values of all channels at a given time sample.
- Theoretically, Reduce memory usage down to 18%.
- Can still be parallelized over the 16 different filters
*** Transformation in convolution, do not store the 32bit result but the transformed 8bit value (after BatchNorm)
** Questions
*** Is it possible for multiple DMAs at the same time? and can we wait for a specific one to complete?
A: Yes, but XIAYING will check
*** Xpulp V2 has instructions for rounding (adding offset, and shifting), but not with division.
- Also, in Python, integer division always rounds towards negative, while C division rounds towards 0.
- With round towards negative, we can just add half the division factor, and the number is rounded.
- How I do it: 
  1. extract the sign bit
  2. Add the bias + half division factor in all cases
  3. add sign bit * -division factor, thus change offset to negative only if the sign bit is set
Is there a setting where this behavior can be changed? (round towards negative)

A: Try to modify quantlab STE to round towards zero
*** Trace of runtime errors? Which instruction caused the exception?
A: use make clean all CONFIG_OPT=gvsoc/trace=insn run (check gvsoc doc)
*** Measure time pulp waits for DMA transfer (Is cycles count while waiting for DMA accurate?)
A: Use GTK Wave
*** Ri5cy core has 4 stages: IF, ID, EX, WB. Data dependency results in stall? How does two cycle latency come into play here?
A: Ri5cy can loop data directly from wb to ex, thus no stall.
*** Documentation for makefile (pulp_rt.mk). How can I link object files to speed up test functions
A: Check the make files in the SDK
*** Why does the transform function take much more cycles than instructions? 
A: Check with GTK Wave, why it uses more cycles
** Next Steps
*** Implement naiive approach
*** Compare with Pulp-NN
*** Quantize further, check when the thing breaks down
*** Try to round down in quantlab, such that it has the same behavior as C integer division
