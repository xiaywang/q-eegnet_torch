* Meeting 19.12.2019
SCHEDULED: <2019-12-19 Thu>
** Preparation
*** TODO EEGNet: Reach same accuracy as Keras (71%)
*** DONE Test out all the different possibilities and write them down.
** Progress (What I have tried)
[[file:results.org][results.org]]
*** In depth comparison of the model
**** Updated Batch Normalization Momentum to match Keras. No significant change
**** Same epsilon for Batch Normalization and Adam. No significant change
*** Use of ~SpatialDropout2D~
**** No significant change for the Keras model.
**** Interestingly: When using ~SpatialDropout2D~ with ~channels_last~ as data format (instead of ~channels_first~), got an improvement of more than 1% (reaching *72.3% +- 3.1%*)
**** For Torch model, performance decreased with regular ~SpatialDropout2D~, but stayed about the same when using the wrong ~channels_last~ format.
*** Using no ~BatchNormalization~ and compared the results
**** Torch model achieves 2% better accuracy than the Keras model (reaching 65.2%)
*** Using ~SDG~ instead of ~Adam~, with ~lr=0.01~
**** Torch model achieves 3% better accuracy than the Keras model (reaching 67.6%)
*** Exporting the model from Keras, transforming the matrices and importing into Torch
**** Not working (yet), got a result corresponding to random guessing (25%)
**** Small example of transforming convolutional and linear layers are working (tested with l2 norm)
**** DONE Test the same when combining multiple layers, extend to small dummy EEGNet without training to find the difference
Both models are actually equal. Thus, training must be different.
** Next Steps:
*** Stuff to try in order to achieve the same accuracy:
**** Which dropout makes the difference
**** Is flatten really different?
***** Answer: ~channels_last~ does make the difference. Added ~PermutedFlatten~, which does:
#+BEGIN_SRC python
def forward(input):
    return input.permute(0, 2, 3, 1).flatten()
#+END_SRC
But Does it make a difference? *No*
**** Average training curves and compare
**** Use SGD + ~ReduceLROnPlateau~
*** Start Quantization with reference value reached in Torch
**** Linear Quantization
**** 8bit for both the weights and the activations
**** Integrate in QuantLab, instead of the Quantization framework of Torch
** Weekly Meeting: Every Wendsday at 14:00, starting from 08.01.2020


